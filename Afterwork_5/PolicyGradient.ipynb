{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Policy Gradient\n",
    "Un exemple simple de *policy gradient* sur l'environnement OpenAI Gym `LunarLander-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On définit d'abord quelques constantes.\n",
    "\n",
    "`LEARNING_RATE`: classique, contrôle la taille des pas de la descente de gradient.\n",
    "\n",
    "`BATCH_SIZE`: Nombre d'épisodes (trajectoires) qu'on génère entre chaque étape d'apprentissage.\n",
    "\n",
    "`EPOCHS`: Nombre de fois qu'on répète l'étape d'apprentissage.\n",
    "\n",
    "`MAX_STEPS`: Permet de limiter la durée d'un épisode, on stoppe les épisodes trop long.\n",
    "\n",
    "`MIN_REWARD`: De même, si un épisode atteint une récompense si basse, on le stoppe.\n",
    "\n",
    "`GAMMA`: Permet de régler l'importance d'une récompense immédiate par rapport à une même récompense dans le futur. Une récompense dans $n$ transitions à partir d'un état donné sera multiplié par $\\gamma^n$ pour l'estimation de la récompense future de cet état.\n",
    "\n",
    "`HIDDEN_LAYER_WIDTH`: Largeur de notre petit réseau de neurone qui va estimer la stratégie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 15\n",
    "EPOCHS = 2000\n",
    "MAX_STEPS = 600\n",
    "MIN_REWARD = -350\n",
    "GAMMA = 0.99\n",
    "HIDDEN_LAYER_WIDTH = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Number of actions: 4\n",
      "Shape of the state: (8,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "\n",
    "print(\"Number of actions: {}\".format(a_size))\n",
    "print(\"Shape of the state: {}\".format(env.observation_space.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de notre policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholder qui contiendra une liste de vecteurs d'états\n",
    "with tf.variable_scope(\"placeholders\"):\n",
    "    states_plh = tf.placeholder(shape=[None, s_size], dtype=tf.float32, name=\"states\")\n",
    "\n",
    "# Couche intermédiaire du réseau de neurones (activation ReLU)\n",
    "with tf.variable_scope(\"hidden_layer\"):\n",
    "    weights_l1 = tf.get_variable(\"weights\", shape=[s_size, HIDDEN_LAYER_WIDTH], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases_l1 = tf.get_variable(\"biases\", shape=[HIDDEN_LAYER_WIDTH], initializer=tf.zeros_initializer)\n",
    "    out_l1 = tf.nn.relu(tf.matmul(states_plh, weights_l1) + biases_l1)\n",
    "\n",
    "# Couche de sortie\n",
    "with tf.variable_scope(\"outputs\"):\n",
    "    weights_l2 = tf.get_variable(\"weights\", shape=[HIDDEN_LAYER_WIDTH, a_size], initializer=tf.contrib.layers.xavier_initializer(1))\n",
    "    biases_l2 = tf.get_variable(\"biases\", shape=[a_size], initializer=tf.zeros_initializer)\n",
    "\n",
    "    # Liste de vecteurs de probabilités (probabilité de chaque action)\n",
    "    policy_probs = tf.nn.softmax(tf.matmul(out_l1, weights_l2) + biases_l2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"placeholders\"):\n",
    "    # Au moment de l'entrainement, on a besoin de spécifier la liste des actions prise,\n",
    "    # et la liste des récompenses futures obtenues (ajustées avec GAMMA)\n",
    "    actions_plh = tf.placeholder(shape=[None,], dtype=tf.int32, name=\"actions\")\n",
    "    rewards_plh = tf.placeholder(shape=[None,], dtype=tf.float32, name=\"rewards\")\n",
    "\n",
    "# Depuis les vecteurs de probabilités, sélectionne uniquement les valeurs qui ont mené à l'action associée\n",
    "responsible_probs = tf.reduce_sum(policy_probs * tf.one_hot(actions_plh, depth=a_size), axis=1)\n",
    "\n",
    "# Objectif à maximiser, pondéré par les recompenses données\n",
    "objective = tf.reduce_mean(tf.log(responsible_probs) * rewards_plh)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(-objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted rewards\n",
    "A partir du vecteur des récompenses, on calcule pour chaque élément la future récompense obtenue à partir de cet état. Plus une récompense est dans le future, plus elle est réduite (en fonction de $\\gamma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward(rewards):\n",
    "    ret = []\n",
    "    running_r = 0\n",
    "    rewards -= np.mean(rewards)\n",
    "    for r in reversed(rewards):\n",
    "        running_r = running_r * GAMMA + r\n",
    "        ret.append(running_r)\n",
    "        \n",
    "    ret = np.asarray(list(reversed(ret)))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération de trajectoire\n",
    "Pour chaque état, on utilise notre réseau de neurones pour obtenir une distribution des probabilités de chaque action. On choisit l'action à prendre en suivant cette distribution. A chaque pas, on stocke l'état précédent, l'action effectuée, l'état suivant, et la récompense obtenue dans une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def run_trajectory(render=False):\n",
    "    s = env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    traj = []\n",
    "    \n",
    "    for _ in range(MAX_STEPS):\n",
    "        probs = sess.run(policy_probs, feed_dict={states_plh: [s]})[0]\n",
    "        a = np.random.choice(range(a_size), p=probs)\n",
    "        \n",
    "        s1, r, d, _ = env.step(a)\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        traj.append([s, a, s1, r])\n",
    "        \n",
    "        total_reward += r\n",
    "        \n",
    "        s = s1\n",
    "        \n",
    "        if total_reward < MIN_REWARD:\n",
    "            break\n",
    "    \n",
    "    return (np.array(traj), total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancement de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# saver.restore(sess, \"./save-ckpt/ckpt\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    s = env.reset()\n",
    "    trajectories = []\n",
    "    rewards = []\n",
    "    total_rewards = []\n",
    "    \n",
    "    for batch in range(BATCH_SIZE):\n",
    "        traj, r = run_trajectory(False)\n",
    "        trajectories.append(traj)\n",
    "        \n",
    "        discounted = discounted_reward(traj[:, 3])\n",
    "        rewards.append(discounted)\n",
    "        \n",
    "        total_rewards.append(r)\n",
    "        \n",
    "    # Accumule les trajectoires\n",
    "    all_trajs = np.vstack(trajectories)\n",
    "    \n",
    "    # Accumule les récompenses, puis les centre et les normalise (permet de réduire la variance de la policy)\n",
    "    all_rewards = np.hstack(rewards)\n",
    "    all_rewards -= np.mean(all_rewards)\n",
    "    all_rewards /= np.std(all_rewards)\n",
    "        \n",
    "    feed_dict = {\n",
    "                states_plh: np.vstack(all_trajs[:, 0]),\n",
    "                actions_plh: all_trajs[:,1],\n",
    "                rewards_plh: all_rewards\n",
    "    }\n",
    "    \n",
    "    # Lance une étape de descente de gradient\n",
    "    _ = sess.run([train_op], feed_dict=feed_dict)\n",
    "    \n",
    "    saver.save(sess, \"save-ckpt/ckpt\")\n",
    "    \n",
    "    print(\"Epoch {} finished. Mean reward: {}\".format(epoch, np.mean(total_rewards)))\n",
    "          \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver.restore(sess, \"./save-ckpt/ckpt\")\n",
    "\n",
    "for _ in range(100):\n",
    "    s = env.reset()\n",
    "    while True:\n",
    "        probs = sess.run(policy_probs, feed_dict={states_plh: [s]})[0]\n",
    "        a = np.random.choice(range(a_size), p=probs)\n",
    "        env.render()\n",
    "        s, r, d, _ = env.step(a)\n",
    "\n",
    "        if d:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Génère un GIF ! (actions aléatoires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a GIF for random actions\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "\n",
    "NUM_RUNS = 10\n",
    "\n",
    "# Police dans le même répertoire\n",
    "font = ImageFont.truetype(\"Ubuntu.ttf\", 40)\n",
    "\n",
    "i = 0\n",
    "for _ in range(5):\n",
    "    s = env.reset()\n",
    "    running_r = 0\n",
    "    \n",
    "    while True:\n",
    "        img = env.render(mode='rgb_array')\n",
    "        \n",
    "        # Ecriture du score\n",
    "        img = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.text((20, 20),\"Score: {:5.2f}\".format(running_r),(255,255,255),font=font)\n",
    "        \n",
    "        img.save('frame_{:04}.jpg'.format(i))\n",
    "        env.render()\n",
    "        \n",
    "        probs = sess.run(policy_probs, feed_dict={states_plh: [s]})[0]\n",
    "        a = np.random.choice(range(a_size), p=probs)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        running_r += r\n",
    "        i += 1\n",
    "\n",
    "        if d:   \n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
